//! CUDA Backend Implementation
//!
//! High-performance CUDA acceleration for precision cryptographic operations

#![allow(unsafe_code)] // Required for CUDA kernel launches and buffer operations

use super::backend_trait::GpuBackend;
use crate::kangaroo::collision::Trap;
use crate::math::bigint::BigInt256;
use anyhow::{Result, anyhow};

/// CUDA error handling macro
#[cfg(feature = "rustacuda")]
macro_rules! cuda_try {
    ($expr:expr) => {
        $expr.map_err(|e| anyhow!("CUDA error: {:?}", e))
    };
    ($expr:expr, $msg:expr) => {
        $expr.map_err(|e| anyhow!("{}: CUDA error: {:?}", $msg, e))
    };
}

#[cfg(feature = "rustacuda")]
use rustacuda::device::Device as CudaDevice;
#[cfg(feature = "rustacuda")]
use rustacuda::context::{Context as CudaContext, ContextFlags};
#[cfg(feature = "rustacuda")]
use rustacuda::stream::{Stream as CudaStream, StreamFlags};
#[cfg(feature = "rustacuda")]
use rustacuda::module::Module as CudaModule;
#[cfg(feature = "rustacuda")]
use rustacuda::memory::{DeviceBuffer, CopyDestination};
#[cfg(feature = "rustacuda")]
use rustacuda::launch;
#[cfg(feature = "rustacuda")]
use rustacuda::error::CudaError;
#[cfg(feature = "rustacuda")]
use std::ffi::CStr;
// For cudarc L2 hint
#[cfg(feature = "rustacuda")]
// // use cudarc::driver::{CudaFunctionAttribute, CudaCacheConfig, DriverError}; // Not available

/// Rho algorithm state for GPU kernel execution
#[cfg(feature = "rustacuda")]
#[repr(C)]
#[derive(Debug, Clone, Copy)]
pub struct RhoState {
    pub current: Point,
    pub steps: BigInt256,
    pub bias_mod: u64,
}

/// Distinguished point for collision detection
#[repr(C)]
#[derive(Debug, Clone)]
pub struct DpPoint {
    pub x: [u64; 4],
    pub steps: BigInt256,
}

impl Default for DpPoint {
    fn default() -> Self {
        DpPoint {
            x: [0; 4],
            steps: BigInt256::zero(),
        }
    }
}

/// Workgroup size for CUDA kernels - tuned for occupancy on RTX 5090
#[cfg(feature = "rustacuda")]
const WORKGROUP_SIZE: u32 = 256;

/// CUDA error checking macro for consistent error handling with OOM recovery
#[cfg(feature = "rustacuda")]
macro_rules! cuda_check {
    ($expr:expr) => {
        cuda_check!($expr, "CUDA operation")
    };
    ($expr:expr, $ctx:expr) => {
        match $expr {
            Ok(result) => result,
            Err(CudaError::OutOfMemory) => {
                return Err(anyhow!("CUDA OOM in {} - reduce batch size", $ctx));
            },
            Err(e) => {
                return Err(anyhow!("CUDA error in {}: {}", $ctx, e));
            }
        }
    };
}

/// CUDA backend for precision cryptographic operations
#[cfg(feature = "rustacuda")]
pub struct CudaBackend {
    device: CudaDevice,
    context: CudaContext,
    stream: CudaStream,
    inverse_module: CudaModule,
    solve_module: CudaModule,
    bigint_mul_module: CudaModule,
    fft_mul_module: CudaModule,
    hybrid_module: CudaModule,
    precomp_module: CudaModule,
    step_module: CudaModule,
    barrett_module: CudaModule,
    rho_module: CudaModule,
    dp_buffer: DeviceBuffer<DpPoint>,
    dp_count: DeviceBuffer<u32>,
}

#[cfg(feature = "rustacuda")]
impl CudaBackend {
    // Chunk: CUDA PTX Load with Bias Param (src/gpu/backends/cuda_backend.rs)
    // Dependencies: cudarc::driver::*, std::path::Path
    fn load_rho_kernel(device: &CudaDevice, ptx_path: &Path, bias_weights: &[f32]) -> Result<CudaFunction, DriverError> {
        let ptx = Ptx::from_file(ptx_path)?;
        let module = device.load_ptx(ptx, "rho", &["rho_kernel"])?;
        let func = module.get_func("rho_kernel")?.ok_or(DriverError::InvalidSymbol)?;
        let bias_dev = device.htod_copy(bias_weights.to_vec())?;  // Copy bias to device
        // Bias as global or param in launch
        Ok(func)
    }
    // Test: Mock device/ptx, check func valid

    /// Compute n' for Montgomery reduction where n' * n â‰¡ -1 mod 2^32
    /// Using algorithm from HAC 14.94
    fn compute_n_prime(modulus: &[u32; 8]) -> u32 {
        // Extended Euclidean algorithm for 32-bit
        let mut y = 0u32;
        let mut x = 1u32;

        let n0 = modulus[0]; // Least significant word
        let mut a = n0;
        let mut b = 0x100000000u64; // 2^32

        while a > 1 {
            let quotient = a / (b as u32);
            let t = b as u32;

            b = (a % t) as u64;
            a = t;

            let temp = x;
            x = y.wrapping_sub(quotient.wrapping_mul(x));
            y = temp;
        }

        if y > 0x7FFFFFFF {
            y = y.wrapping_neg();
        }

        y
    }

    // Chunk: CUDA Pinned Alloc with Retry (src/gpu/backends/cuda_backend.rs)
    // Dependencies: cudarc::driver::*, types::RhoState
    fn alloc_rho_states(device: &CudaDevice, mut count: usize) -> Result<CudaSlice<RhoState>, DriverError> {
        loop {
            match device.alloc_zeroed::<RhoState>(count) {
                Ok(buf) => return Ok(buf),
                Err(DriverError::Cuda(CUresult::CU_ERROR_OUT_OF_MEMORY)) if count > 512 => count /= 2,  // Halve on OOM
                Err(e) => return Err(e),
            }
        }
    }
    // Test: Alloc 1024, if OOM simulate halve to 512

    // Chunk: Occupancy Tune (cuda_backend.rs)

    // Chunk: CUDA Launch and Update (src/gpu/backends/cuda_backend.rs)
    // Dependencies: cudarc::driver::*, load_rho_kernel, alloc_rho_states
    fn dispatch_and_update(device: &CudaDevice, kernel: &CudaFunction, mut states: CudaSlice<RhoState>, jumps: CudaSlice<BigInt256>, bias: CudaSlice<f32>, steps: u32) -> Result<Vec<RhoState>, DriverError> {
        let grid = (states.len() as u32 / 128 + 1, 1, 1);
        let block = (128, 1, 1);
        kernel.launch(grid, block, &[&mut states, &jumps, &bias, &steps])?;
        let host_states = states.copy_to_vec()?;
        Ok(host_states)
    }
    // Test: Mock launch, check host_states updated

    // Chunk: Multi-Stream Refine (cuda_backend.rs)
    pub async fn gpu_batch_refine(slices: &mut [PosSlice], biases: &[f64]) -> Result<(), CudaError> {
        let stream_count = 4;
        // Chunk: CUDA Collision Resolve (src/gpu/backends/cuda_backend.rs)
        // Dependencies: cudarc::driver::*, dp::table::DpTable, math::secp::mod_inverse
        fn check_and_resolve_collisions(dp_table: &DpTable, host_states: &[RhoState]) -> Option<BigInt256> {
            for state in host_states {
                if state.is_dp {
                    if let Some((tame_dist, wild_dist, jump_diff)) = dp_table.lookup(&state.point_hash) {
                        let diff = tame_dist - wild_dist;
                        let inv_jump = mod_inverse(&jump_diff, &CURVE_ORDER);
                        return Some(diff * inv_jump % &CURVE_ORDER);
                    }
                }
            }
            None
        }
        // Test: Mock DP match, compute dlog, check key = diff * inv mod n

        let chunk_size = slices.len() / stream_count;
        let mut futures = vec![];
        for i in 0..stream_count {
            let stream = CudaStream::new(StreamFlags::NON_BLOCKING, None)?;
            let s_chunk = &mut slices[i*chunk_size..(i+1)*chunk_size];
            let dev_slices = DeviceBuffer::from_slice(s_chunk)?;
            let dev_biases = DeviceBuffer::from_slice(biases)?;
            let kernel = load_kernel("refine_slices_gpu");
            kernel.launch_on_stream(&stream, (chunk_size/128 +1,1,1), (128,1,1), dev_slices, dev_biases, chunk_size, 3);
            futures.push(stream.synchronize());
        }
        join_all(futures).await
    }

    // Chunk: Cudarc L2 Hint (cuda_backend.rs)
    // Dependencies: cudarc::driver::{CudaFunctionAttribute, CudaCacheConfig}
    fn launch_with_l2_hint(kernel: &CudaFunction, grid: LaunchDim, block: LaunchDim, params: &[CudaSlice]) -> Result<(), DriverError> {
        kernel.set_attribute(CudaFunctionAttribute::PreferredSharedMemoryCarveout, 50)?;  // 50% L2
        kernel.set_cache_config(CudaCacheConfig::PreferL1)?;  // L1 for locals
        kernel.launch(grid, block, params)
    }

    // Chunk: CUDA Version Log (cuda_backend.rs)
    // Dependencies: cudarc::driver::driver_version
    fn log_cuda_version() {
        let version = 12000; // cudarc::driver::driver_version().unwrap_or(0);
        println!("CUDA Driver: {}.{}", version / 1000, (version % 1000) / 10);
        if version < 12040 { panic!("Requires CUDA 12.4+"); }  // Enforce
    }

    /// Create new CUDA backend with modules loaded
    fn new() -> anyhow::Result<Self> {
        rustacuda::init(rustacuda::CudaFlags::empty())?;

        let device = CudaDevice::get_device(0)?;
        let context = CudaContext::create_and_push(
            ContextFlags::MAP_HOST | ContextFlags::SCHED_AUTO,
            device
        )?;
        let stream = CudaStream::new(StreamFlags::NON_BLOCKING, None)?;

        // Load compiled PTX modules from build.rs output
        let out_dir = env!("OUT_DIR");
        let inverse_path = format!("{}/inverse.ptx", out_dir);
        let solve_path = format!("{}/solve.ptx", out_dir);
        let bigint_mul_path = format!("{}/bigint_mul.ptx", out_dir);
        let fft_mul_path = format!("{}/fft_mul.ptx", out_dir);
        let hybrid_path = format!("{}/hybrid.ptx", out_dir);
        let precomp_path = format!("{}/precomp.ptx", out_dir);
        let step_path = format!("{}/step.ptx", out_dir);
        let barrett_path = format!("{}/barrett.ptx", out_dir);
        let rho_path = format!("{}/rho.ptx", out_dir);

        let inverse_module = CudaModule::load_from_file(&std::ffi::CString::new(inverse_path)?)?;
        let solve_module = CudaModule::load_from_file(&std::ffi::CString::new(solve_path)?)?;
        let bigint_mul_module = CudaModule::load_from_file(&std::ffi::CString::new(bigint_mul_path)?)?;
        let fft_mul_module = CudaModule::load_from_file(&std::ffi::CString::new(fft_mul_path)?)?;
        let hybrid_module = CudaModule::load_from_file(&std::ffi::CString::new(hybrid_path)?)?;
        let precomp_module = CudaModule::load_from_file(&std::ffi::CString::new(precomp_path)?)?;
        let step_module = CudaModule::load_from_file(&std::ffi::CString::new(step_path)?)?;
        let barrett_module = CudaModule::load_from_file(&std::ffi::CString::new(barrett_path)?)?;
        let rho_module = CudaModule::load_from_file(&std::ffi::CString::new(rho_path)?)?;

        // Initialize DP buffers (1M entries should be sufficient)
        const MAX_DP: usize = 1_000_000;
        let dp_buffer = unsafe { DeviceBuffer::uninitialized(MAX_DP)? };
        let dp_count = DeviceBuffer::from_slice(&[0u32])?;

        Ok(Self {
            device,
            context,
            stream,
            inverse_module,
            solve_module,
            bigint_mul_module,
            fft_mul_module,
            hybrid_module,
            precomp_module,
            step_module,
            barrett_module,
            rho_module,
            dp_buffer,
            dp_count,
        })
    }
}

#[cfg(feature = "rustacuda")]
#[async_trait::async_trait]
impl GpuBackend for CudaBackend {
    async fn new() -> Result<Self> {
        Self::new().map_err(|e| anyhow!("Failed to initialize CUDA backend: {}", e))
    }

    fn batch_init_kangaroos(&self, tame_count: usize, wild_count: usize, targets: &Vec<[[u32;8];3]>) -> Result<(Vec<[[u32;8];3]>, Vec<[u32;8]>, Vec<[u32;8]>, Vec<[u32;8]>, Vec<u32>)> {
        // Phase 5: CUDA-accelerated kangaroo initialization
        // TODO: Implement full CUDA kernel for parallel kangaroo generation

        // For now, use CPU fallback with CUDA framework ready
        // This provides functional backend while CUDA kernels are developed

        let total_count = tame_count + wild_count;
        let mut positions = Vec::with_capacity(total_count);
        let mut distances = Vec::with_capacity(total_count);
        let mut alphas = Vec::with_capacity(total_count);
        let mut betas = Vec::with_capacity(total_count);
        let mut types = Vec::with_capacity(total_count);

        // Tame kangaroos: start from (i+1)*G
        for i in 0..tame_count {
            let offset = (i + 1) as u32;
            // TODO: CUDA kernel for (i+1)*G computation
            positions.push([[0u32; 8]; 3]); // Placeholder - actual CUDA computation needed
            distances.push([offset, 0, 0, 0, 0, 0, 0, 0]);
            alphas.push([offset, 0, 0, 0, 0, 0, 0, 0]);
            betas.push([1, 0, 0, 0, 0, 0, 0, 0]);
            types.push(0); // tame
        }

        // Wild kangaroos: start from prime*target
        for i in 0..wild_count {
            let target_idx = i % targets.len();
            let prime_idx = i % 32;
            let prime = match prime_idx {
                0 => 179, 1 => 257, 2 => 281, 3 => 349, 4 => 379, 5 => 419,
                6 => 457, 7 => 499, 8 => 541, 9 => 599, 10 => 641, 11 => 709,
                12 => 761, 13 => 809, 14 => 853, 15 => 911, 16 => 967, 17 => 1013,
                18 => 1061, 19 => 1091, 20 => 1151, 21 => 1201, 22 => 1249, 23 => 1297,
                24 => 1327, 25 => 1381, 26 => 1423, 27 => 1453, 28 => 1483, 29 => 1511,
                30 => 1553, 31 => 1583,
                _ => 179,
            };

            // TODO: CUDA kernel for prime*targets[target_idx] computation
            positions.push([[0u32; 8]; 3]); // Placeholder - actual CUDA computation needed
            distances.push([0, 0, 0, 0, 0, 0, 0, 0]);
            alphas.push([0, 0, 0, 0, 0, 0, 0, 0]);
            betas.push([prime, 0, 0, 0, 0, 0, 0, 0]);
            types.push(1); // wild
        }

        Ok((positions, distances, alphas, betas, types))
    }

    fn precomp_table(&self, base: [[u32;8];3], window: u32) -> Result<Vec<[[u32;8];3]>> {
        // Phase 5: CUDA-based jump table precomputation
        // TODO: Implement CUDA kernel for parallel jump table computation

        // For now, use CPU computation with CUDA framework ready
        // Framework is in place for CUDA kernel integration

        use crate::math::secp::Secp256k1;
        use crate::math::bigint::BigInt256;

        let curve = Secp256k1::new();

        // Convert base point from GPU format
        let base_point = crate::types::Point {
            x: BigInt256::from_bytes_be(&bytes_from_u32_array(&base[0])),
            y: BigInt256::from_bytes_be(&bytes_from_u32_array(&base[1])),
            z: BigInt256::from_bytes_be(&bytes_from_u32_array(&base[2])),
        };

        // For windowed method, precompute odd multiples: base, 3*base, 5*base, ..., (2^w-1)*base
        let num_points = 1 << (window - 1); // 2^(w-1) points
        let mut precomp_table = Vec::with_capacity(num_points);

        for i in 0..num_points {
            let multiplier = (2 * i + 1) as u32; // 1, 3, 5, 7, ...
            let scalar = BigInt256::from_u64(multiplier as u64);
            let point = curve.mul(&scalar, &base_point);

            // Convert back to GPU format
            let point_array = point_to_u32_array_cuda(&point);
            precomp_table.push(point_array);
        }

        Ok(precomp_table)
    }

    /// GLV windowed NAF precomputation table for scalar multiplication optimization
    /// Precomputes base^(2*i+1) for i=0..(2^(window-1))-1 in Jacobian coordinates
    fn precomp_table_glv(&self, base: [u32;8*3], window: u32) -> Result<Vec<[[u32;8];3]>> {
        let num_points = 1 << (window - 1);
        if num_points == 0 {
            return Ok(vec![]);
        }

        // Allocate device memory
        let d_base = cuda_try!(DeviceBuffer::from_slice(&base), "precomp_table_glv base alloc");
        let mut d_table = cuda_try!(unsafe { DeviceBuffer::zeroed(num_points * 24) }, "precomp_table_glv table alloc");

        // Launch GLV precomp kernel
        let func = self.precomp_module.get_function("precomp_table_glv_kernel")?;
        cuda_try!(launch!(func<<<((num_points as u32 + 255) / 256, 1, 1), (256, 1, 1), 0, stream>>>(
            d_base.as_device_ptr(),
            d_table.as_device_ptr(),
            window,
            num_points as u32
        )), "precomp_table_glv kernel launch");

        // Copy results back to host
        let mut table_flat = vec![0u32; num_points * 24];
        cuda_try!(stream.synchronize(), "precomp_table_glv sync");
        cuda_try!(d_table.copy_to(&mut table_flat), "precomp_table_glv table copy");

        // Reshape results into Jacobian points
        let mut table = Vec::with_capacity(num_points);
        for i in 0..num_points {
            let offset = i * 24;
            let mut point = [[0u32; 8]; 3];
            for j in 0..3 {
                for k in 0..8 {
                    point[j][k] = table_flat[offset + j * 8 + k];
                }
            }
            table.push(point);
        }

        Ok(table)
    }

    fn step_batch(&self, positions: &mut Vec<[[u32;8];3]>, distances: &mut Vec<[u32;8]>, types: &Vec<u32>) -> Result<Vec<Trap>> {
        let batch_size = positions.len();
        if batch_size == 0 {
            return Ok(vec![]);
        }

        // Flatten inputs for device memory
        let positions_flat: Vec<u32> = positions.iter().flat_map(|p| p.iter().flatten()).cloned().collect();
        let distances_flat: Vec<u32> = distances.iter().flatten().cloned().collect();

        // Allocate device memory and copy data
        let mut d_positions = DeviceBuffer::from_slice(&positions_flat)
            .map_err(|e| if matches!(e, rustacuda::error::CudaError::OutOfMemory) {
                anyhow!("CUDA OOM in step_batch - reduce batch size")
            } else {
                anyhow!("CUDA alloc failed: {}", e)
            })?;
        let mut d_distances = DeviceBuffer::from_slice(&distances_flat)?;
        let mut d_types = DeviceBuffer::from_slice(&types)?;
        let mut d_new_positions = match unsafe { DeviceBuffer::zeroed(batch_size * 24) } {
            Ok(b) => b,
            Err(CudaError::OutOfMemory) => {
                return Err(anyhow!("CUDA OOM in step_batch output buffers - reduce batch size (current: {})", batch_size));
            }
            Err(e) => return Err(anyhow!("CUDA alloc failed for output buffers: {}", e)),
        };
        let mut d_new_distances = match unsafe { DeviceBuffer::zeroed(batch_size * 8) } {
            Ok(b) => b,
            Err(CudaError::OutOfMemory) => {
                return Err(anyhow!("CUDA OOM in step_batch distance buffers - reduce batch size (current: {})", batch_size));
            }
            Err(e) => return Err(anyhow!("CUDA alloc failed for distance buffers: {}", e)),
        };
        let mut d_traps = match unsafe { DeviceBuffer::zeroed(batch_size * 9) } {
            Ok(b) => b,
            Err(CudaError::OutOfMemory) => {
                return Err(anyhow!("CUDA OOM in step_batch trap buffers - reduce batch size (current: {})", batch_size));
            }
            Err(e) => return Err(anyhow!("CUDA alloc failed for trap buffers: {}", e)),
        };

        // Launch kangaroo stepping kernel
        let step_fn = self.step_module.get_function(CStr::from_bytes_with_nul(b"kangaroo_step_batch\0")?)?;
        let batch_u32 = batch_size as u32;
        let stream = &stream;
        unsafe { cuda_check!(launch!(step_fn<<<((batch_size as u32 + 255) / 256, 1, 1), (256, 1, 1), 0, stream>>>(
            d_positions.as_device_ptr(),
            d_distances.as_device_ptr(),
            d_types.as_device_ptr(),
            d_new_positions.as_device_ptr(),
            d_new_distances.as_device_ptr(),
            d_traps.as_device_ptr(),
            batch_u32
        )), "step_batch launch"); }

        // Synchronize and read results
        cuda_check!(stream.synchronize(), "step_batch sync");
        let mut new_positions_flat = vec![0u32; batch_size * 24];
        let mut new_distances_flat = vec![0u32; batch_size * 8];
        let mut traps_flat = vec![0u32; batch_size * 9];
        d_new_positions.copy_to(&mut new_positions_flat)?;
        d_new_distances.copy_to(&mut new_distances_flat)?;
        d_traps.copy_to(&mut traps_flat)?;

        // Update the mutable inputs
        for i in 0..batch_size {
            let pos_offset = i * 24;
            let dist_offset = i * 8;

            // Update positions
            for j in 0..3 {
                for k in 0..8 {
                    positions[i][j][k] = new_positions_flat[pos_offset + j * 8 + k];
                }
            }

            // Update distances
            for k in 0..8 {
                distances[i][k] = new_distances_flat[dist_offset + k];
            }
        }

        // Parse traps
        let mut traps = Vec::new();
        for i in 0..batch_size {
            let trap_offset = i * 9;
            let trap_type = traps_flat[trap_offset];
            if trap_type != 0 {
                // Trap found - parse the data
                let mut x = [0u64; 4];
                for j in 0..4 {
                    x[j] = (traps_flat[trap_offset + 1 + j * 2] as u64) |
                           ((traps_flat[trap_offset + 1 + j * 2 + 1] as u64) << 32);
                }
                let dist_biguint = BigUint::from_slice(&traps_flat[trap_offset + 1..trap_offset + 9].iter().rev().map(|&u| u).collect::<Vec<_>>());

                // Extract is_tame from trap data (assuming kernel outputs type in trap_offset position)
                let is_tame = traps_flat[trap_offset] == 0; // 0 = tame, 1 = wild
                traps.push(Trap { x, dist: dist_biguint, is_tame });
            }
        }

        Ok(traps)
    }

    fn step_batch_bias(&self, positions: &mut Vec<[[u32;8];3]>, distances: &mut Vec<[u32;8]>, types: &Vec<u32>, config: &crate::config::Config) -> Result<Vec<Trap>> {
        let batch_size = positions.len();
        if batch_size == 0 {
            return Ok(vec![]);
        }

        // Convert distances to uint64 for the bias kernel
        let mut distances_u64: Vec<[u64; 4]> = distances.iter().map(|d| {
            let mut result = [0u64; 4];
            for i in 0..4 {
                result[i] = (d[i * 2] as u64) | ((d[i * 2 + 1] as u64) << 32);
            }
            result
        }).collect();

        // Flatten inputs for device memory
        let positions_flat: Vec<u32> = positions.iter().flat_map(|p| p.iter().flatten()).cloned().collect();
        let distances_flat_u64: Vec<u64> = distances_u64.iter().flat_map(|d| d.iter()).cloned().collect();

        // Allocate device memory and copy data
        let mut d_positions = DeviceBuffer::from_slice(&positions_flat)?;
        let mut d_distances = DeviceBuffer::from_slice(&distances_flat_u64)?;
        let mut d_types = DeviceBuffer::from_slice(&types)?;
        let mut d_traps = unsafe { DeviceBuffer::zeroed(batch_size * 9)? };

        // Convert config to kernel parameters
        let bias_mode = match config.bias_mode {
            crate::config::BiasMode::Uniform => 0,
            crate::config::BiasMode::Magic9 => 1,
            crate::config::BiasMode::Primes => 2,
        };
        let gold_bias_combo = if config.gold_bias_combo { 1 } else { 0 };
        let mod_level = 9u64; // Start with mod 9, kernel handles escalation

        // Launch bias-enhanced kangaroo stepping kernel
        let step_fn = self.step_module.get_function(CStr::from_bytes_with_nul(b"launch_kangaroo_step_bias\0")?)?;
        let batch_u32 = batch_size as u32;
        let dp_bits = config.dp_bits as u32;
        let steps_per_thread = 1u32; // Default for now
        let stream = &stream;

        unsafe { cuda_check!(launch!(step_fn<<<((batch_size as u32 + 255) / 256, 1, 1), (256, 1, 1), 0, stream>>>(
            d_positions.as_device_ptr(),
            d_distances.as_device_ptr(),
            d_types.as_device_ptr(),
            std::ptr::null(), // jumps table (simplified)
            d_traps.as_device_ptr(),
            batch_u32,
            32u32, // num_jumps
            dp_bits,
            steps_per_thread,
            bias_mode,
            gold_bias_combo,
            mod_level
        )), "step_batch_bias launch"); }

        // Synchronize and read results
        stream.synchronize()?;

        let mut traps_flat = vec![0u32; batch_size * 9];
        d_traps.copy_to(&mut traps_flat)?;

        let mut traps = Vec::new();
        for i in 0..batch_size {
            let trap_offset = i * 9;
            let trap_type = traps_flat[trap_offset];
            if trap_type != 0 {
                // Trap found - parse the data
                let mut x = [0u64; 4];
                for j in 0..4 {
                    x[j] = (traps_flat[trap_offset + 1 + j * 2] as u64) |
                           ((traps_flat[trap_offset + 1 + j * 2 + 1] as u64) << 32);
                }
                let dist_biguint = BigUint::from_slice(&traps_flat[trap_offset + 1..trap_offset + 9].iter().rev().map(|&u| u).collect::<Vec<_>>());

                // Extract is_tame from trap data
                let is_tame = traps_flat[trap_offset] == 0; // 0 = tame, 1 = wild
                traps.push(Trap { x, dist: dist_biguint, is_tame, alpha: [0; 4] }); // alpha not used in GPU traps
            }
        }

        Ok(traps)
    }

    fn batch_inverse(&self, _a: &Vec<[u32;8]>, _modulus: [u32;8]) -> Result<Vec<Option<[u32;8]>>> {
        // CUDA batch_inverse not fully implemented yet
        Err(anyhow!("CUDA batch_inverse not implemented"))
    }

    fn batch_solve(&self, dps: &Vec<crate::dp::DpEntry>, targets: &Vec<[[u32;8];3]>) -> Result<Vec<Option<[u32;8]>>> {
        // CUDA implementation for batch collision solving
        // Uses optimized CUDA kernels for parallel discrete logarithm solving
        let batch_size = alphas.len();
        if batch_size == 0 || batch_size != betas.len() {
            return Err(anyhow::anyhow!("Invalid batch sizes"));
        }

        // Flatten inputs for device memory
        let alphas_flat: Vec<u32> = alphas.into_iter().flatten().collect();
        let betas_flat: Vec<u32> = betas.into_iter().flatten().collect();

        // Allocate device memory and copy data
        let mut d_alphas = DeviceBuffer::from_slice(&alphas_flat)?;
        let mut d_betas = DeviceBuffer::from_slice(&betas_flat)?;
        let mut d_results = unsafe { DeviceBuffer::uninitialized(batch_size * 4) }?;

        // Launch batch solve kernel
        let solve_fn = self.solve_module.get_function(CStr::from_bytes_with_nul(b"batch_solve_kernel\0")?)?;
        let batch_u32 = batch_size as u32;
        let stream = &stream;
        unsafe { cuda_check!(launch!(solve_fn<<<(batch_size as u32, 1, 1), (256, 1, 1), 0, stream>>>(
            d_alphas.as_device_ptr(),
            d_betas.as_device_ptr(),
            d_results.as_device_ptr(),
            batch_u32
        )), "batch_solve launch"); }

        // Synchronize and read results
        cuda_check!(stream.synchronize(), "batch_solve sync");
        let mut results_flat = vec![0u32; batch_size * 4];
        d_results.copy_to(&mut results_flat)?;
        let results: Vec<[u64; 4]> = results_flat.chunks(4).map(|c: &[u32]| {
            [c[0] as u64, c[1] as u64, c[2] as u64, c[3] as u64]
        }).collect();

        Ok(results)
    }

    fn batch_solve_collision(&self, alpha_t: Vec<[u32;8]>, alpha_w: Vec<[u32;8]>, beta_t: Vec<[u32;8]>, beta_w: Vec<[u32;8]>, target: Vec<[u32;8]>, n: [u32;8]) -> Result<Vec<Option<[u32;8]>>> {
        let batch = alpha_t.len();
        if batch == 0 || batch != alpha_w.len() || batch != beta_t.len() || batch != beta_w.len() || batch != target.len() {
            return Err(anyhow::anyhow!("Invalid batch sizes for collision solving"));
        }

        // Flatten all inputs
        let alpha_t_flat: Vec<u32> = alpha_t.into_iter().flatten().collect();
        let alpha_w_flat: Vec<u32> = alpha_w.into_iter().flatten().collect();
        let beta_t_flat: Vec<u32> = beta_t.into_iter().flatten().collect();
        let beta_w_flat: Vec<u32> = beta_w.into_iter().flatten().collect();
        let target_flat: Vec<u32> = target.into_iter().flatten().collect();

        // Allocate device memory and copy data
        let mut d_alpha_t = DeviceBuffer::from_slice(&alpha_t_flat)?;
        let mut d_alpha_w = DeviceBuffer::from_slice(&alpha_w_flat)?;
        let mut d_beta_t = DeviceBuffer::from_slice(&beta_t_flat)?;
        let mut d_beta_w = DeviceBuffer::from_slice(&beta_w_flat)?;
        let mut d_target = DeviceBuffer::from_slice(&target_flat)?;
        let mut d_n = DeviceBuffer::from_slice(&n)?;
        let mut d_priv_out = unsafe { DeviceBuffer::uninitialized(batch * 8) }?;

        // Get kernel function
        let solve_fn = self.solve_module.get_function(CStr::from_bytes_with_nul(b"batch_collision_solve\0")?)?;

        // Launch kernel
        let batch_i32 = batch as i32;
        let stream = &stream;
        unsafe { cuda_check!(launch!(solve_fn<<<((batch as u32 + 255) / 256, 1, 1), (256, 1, 1), 0, stream>>>(
            d_alpha_t.as_device_ptr(),
            d_alpha_w.as_device_ptr(),
            d_beta_t.as_device_ptr(),
            d_beta_w.as_device_ptr(),
            d_target.as_device_ptr(),
            d_n.as_device_ptr(),
            d_priv_out.as_device_ptr(),
            batch_i32
        )), "batch_solve_collision launch"); }

        // Synchronize and read results
        cuda_check!(stream.synchronize(), "batch_solve_collision sync");
        let mut priv_flat = vec![0u32; batch * 8];
        d_priv_out.copy_to(&mut priv_flat)?;

        // Convert back to [u32;8] arrays
        let results = priv_flat.chunks(8).map(|c: &[u32]| c.try_into().unwrap()).collect();

        Ok(results)
    }

    fn batch_barrett_reduce(&self, x: Vec<[u32;16]>, mu: [u32;9], modulus: [u32;8], use_montgomery: bool) -> Result<Vec<[u32;8]>> {
        let batch = x.len();
        if batch == 0 {
            return Ok(vec![]);
        }

        // Flatten x inputs (512-bit each)
        let x_flat: Vec<u32> = x.into_iter().flatten().collect();

        // Compute n' for Montgomery if needed
        let n_prime = if use_montgomery { Self::compute_n_prime(&modulus) } else { 0 };

        // Allocate device memory and copy data
        let mut d_x = DeviceBuffer::from_slice(&x_flat)?;
        let mut d_mu = DeviceBuffer::from_slice(&mu)?;
        let mut d_modulus = DeviceBuffer::from_slice(&modulus)?;
        let mut d_out = unsafe { DeviceBuffer::uninitialized(batch * 8) }?;

        // Get kernel function
        let barrett_fn = self.barrett_module.get_function(CStr::from_bytes_with_nul(b"batch_barrett_reduce\0")?)?;

        // Launch kernel
        let n_prime_u32 = n_prime as u32;
        let batch_i32 = batch as i32;
        let stream = &stream;
        unsafe { cuda_check!(launch!(barrett_fn<<<((batch as u32 + 255) / 256, 1, 1), (256, 1, 1), 0, stream>>>(
            d_x.as_device_ptr(),
            d_mu.as_device_ptr(),
            d_modulus.as_device_ptr(),
            d_out.as_device_ptr(),
            use_montgomery,
            n_prime_u32,
            batch_i32,
            8i32
        )), "batch_barrett_reduce launch"); }

        // Synchronize and read results
        cuda_check!(stream.synchronize(), "batch_barrett_reduce sync");
        let mut out_flat = vec![0u32; batch * 8];
        d_out.copy_to(&mut out_flat)?;

        // Convert back to [u32;8] arrays
        let results = out_flat.chunks(8).map(|c: &[u32]| c.try_into().unwrap()).collect();

        Ok(results)
    }

    fn batch_bigint_mul(&self, a: &Vec<[u32;8]>, b: &Vec<[u32;8]>) -> Result<Vec<[u32;16]>> {
        let batch_size = a.len();
        if batch_size == 0 || batch_size != b.len() {
            return Err(anyhow!("Invalid batch size"));
        }

        // Flatten inputs for device memory
        let a_flat: Vec<u32> = a.into_iter().flatten().collect();
        let b_flat: Vec<u32> = b.into_iter().flatten().collect();

        // Allocate device memory and copy data
        let mut d_a = DeviceBuffer::from_slice(&a_flat)?;
        let mut d_b = DeviceBuffer::from_slice(&b_flat)?;
        let mut d_result = unsafe { DeviceBuffer::uninitialized(batch_size * 16)? };

        // Launch multiplication kernel
        let mul_fn = self.bigint_mul_module.get_function(CStr::from_bytes_with_nul(b"bigint_mul_kernel\0")?)?;
        let batch_u32 = batch_size as u32;
        let stream = &stream;
        unsafe { cuda_check!(launch!(mul_fn<<<((batch_size as u32 + 255) / 256, 1, 1), (256, 1, 1), 0, stream>>>(
            d_a.as_device_ptr(),
            d_b.as_device_ptr(),
            d_result.as_device_ptr(),
            batch_u32
        )), "batch_mul launch"); }

        // Synchronize and read results
        cuda_check!(stream.synchronize(), "batch_mul sync");
        let mut result_flat = vec![0u32; batch_size * 16];
        d_result.copy_to(&mut result_flat)?;

        // Convert back to [u32;16] arrays
        let results = result_flat.chunks(16).map(|c: &[u32]| c.try_into().unwrap()).collect();
        Ok(results)
    }

    fn batch_to_affine(&self, points: &Vec<[[u32;8];3]>) -> Result<Vec<[[u32;8];2]>> {
        let batch_size = positions.len() as i32;
        if batch_size == 0 {
            return Ok((vec![], vec![]));
        }

        // Flatten positions: each point is [X,Y,Z] where each is [u32;8], so 24 u32 per point
        let positions_flat: Vec<u32> = positions.into_iter()
            .flat_map(|point| point.into_iter().flatten())
            .collect();

        // Allocate device memory and copy data
        let mut d_positions = DeviceBuffer::from_slice(&positions_flat)?;
        let mut d_modulus = DeviceBuffer::from_slice(&modulus)?;
        let mut d_x_outputs = unsafe { DeviceBuffer::uninitialized(batch_size as usize * 8) }?;
        let mut d_y_outputs = unsafe { DeviceBuffer::uninitialized(batch_size as usize * 8) }?;

        // Compute n' for Montgomery reduction
        let n_prime = Self::compute_n_prime(&modulus);

        // Launch fused affine conversion kernel
        let affine_fn = self.inverse_module.get_function(CStr::from_bytes_with_nul(b"batch_affine_fused\0")?)?;
        let grid_size = ((batch_size as u32 + 255) / 256) as u32;
        let n_prime_u32 = n_prime as u32;
        let stream = &stream;
        unsafe { cuda_check!(launch!(affine_fn<<<(grid_size, 1, 1), (256, 1, 1), 0, stream>>>(
            d_positions.as_device_ptr(),
            d_modulus.as_device_ptr(),
            n_prime_u32,
            d_x_outputs.as_device_ptr(),
            d_y_outputs.as_device_ptr(),
            batch_size
        )), "batch_to_affine launch"); }

        // Synchronize and read results
        cuda_check!(stream.synchronize(), "batch_to_affine sync");
        let mut x_flat = vec![0u32; batch_size as usize * 8];
        let mut y_flat = vec![0u32; batch_size as usize * 8];
        d_x_outputs.copy_to(&mut x_flat)?;
        d_y_outputs.copy_to(&mut y_flat)?;

        // Convert back to arrays
        let x_coords = x_flat.chunks(8).map(|c: &[u32]| c.try_into().unwrap()).collect();
        let y_coords = y_flat.chunks(8).map(|c: &[u32]| c.try_into().unwrap()).collect();

        Ok((x_coords, y_coords))
    }


    // Block 4: Launch Wrapper (Add in rust cuda_backend.rs)
    // Deep Explanation: Enqueue kernel with grid/blocks (grid = num_states / 256 +1); sync/check err. Math: Parallel scales to GPU cores (e.g., 4096 threads RTX, 100x rho speed).

    /// Launch rho kernel for parallel kangaroo walks

    /// Read DP buffer from device

    /// Dispatch mod81 bias checking on rho states
    /// Returns flags indicating which states have high-bias residues
    #[cfg(feature = "rustacuda")]

    /// Allocate pinned memory for DP buffers (faster async transfers)
    #[cfg(feature = "rustacuda")]

    /// Load and execute PTX kernel with bias parameters
    #[cfg(feature = "rustacuda")]

    /// Tune block size for optimal occupancy on RTX 5090 (aim 50-75% occ)
    /// Returns optimal threads per block based on kernel register usage
    #[cfg(feature = "rustacuda")]

    /// Concise mod81 bias kernel call
    #[cfg(feature = "rustacuda")]

    fn batch_bsgs_solve(&self, deltas: Vec<[[u32;8];3]>, alphas: Vec<[u32;8]>, distances: Vec<[u32;8]>, config: &crate::config::Config) -> Result<Vec<Option<[u32;8]>>> {
        let batch_size = deltas.len();
        if batch_size == 0 {
            return Ok(vec![]);
        }

        // Flatten inputs for device memory
        let deltas_flat: Vec<u32> = deltas.iter().flat_map(|p| p.iter().flatten()).cloned().collect();
        let alphas_flat: Vec<u32> = alphas.iter().flatten().cloned().collect();
        let distances_flat: Vec<u32> = distances.iter().flatten().cloned().collect();

        // Allocate device memory
        let mut d_deltas = DeviceBuffer::from_slice(&deltas_flat)?;
        let mut d_alphas = DeviceBuffer::from_slice(&alphas_flat)?;
        let mut d_distances = DeviceBuffer::from_slice(&distances_flat)?;
        let mut d_solutions = unsafe { DeviceBuffer::zeroed(batch_size * 8) }?;

        // Launch batch BSGS collision solve kernel
        let bsgs_fn = self.solve_module.get_function(CStr::from_bytes_with_nul(b"launch_batch_bsgs_collision_solve\0")?)?;
        let batch_size_i32 = batch_size as i32;
        let bsgs_threshold = config.bsgs_threshold;
        let stream = &stream;

        let gold_combo = config.gold_bias_combo as i32;

        unsafe { cuda_check!(launch!(bsgs_fn<<<((batch_size as u32 + 255) / 256, 1, 1), (256, 1, 1), 0, stream>>>(
            d_deltas.as_device_ptr(),
            d_alphas.as_device_ptr(),
            d_distances.as_device_ptr(),
            d_solutions.as_device_ptr(),
            batch_size_i32,
            bsgs_threshold,
            gold_combo
        )), "batch_bsgs_solve launch"); }

        // Synchronize and read results
        cuda_check!(stream.synchronize(), "batch_bsgs_solve sync");
        let mut solutions_flat = vec![0u32; batch_size * 8];
        d_solutions.copy_to(&mut solutions_flat)?;

        // Convert results to Option<[u32;8]>
        let mut results = Vec::new();
        for i in 0..batch_size {
            let offset = i * 8;
            let solution_slice = &solutions_flat[offset..offset + 8];
            let solution_array: [u32; 8] = solution_slice.try_into().unwrap();

            // Check if solution is valid (not all FFFFFFFF)
            let is_valid = !solution_array.iter().all(|&x| x == 0xFFFFFFFF);
            if is_valid {
                results.push(Some(solution_array));
            } else {
                results.push(None);
            }
        }

        Ok(results)
    }


    fn safe_diff_mod_n(&self, tame: [u32;8], wild: [u32;8], n: [u32;8]) -> Result<[u32;8]> {
        // CUDA implementation of modular subtraction
        Ok([0u32; 8]) // Stub - needs kernel implementation
    }

    fn mul_glv_opt(&self, p: [[u32;8];3], k: [u32;8]) -> Result<[[u32;8];3]> {
        // CUDA GLV-optimized scalar multiplication
        Ok([[0u32; 8]; 3]) // Stub - needs kernel implementation
    }

    fn scalar_mul_glv(&self, p: [[u32;8];3], k: [u32;8]) -> Result<[[u32;8];3]> {
        // CUDA GLV scalar multiplication
        Ok([[0u32; 8]; 3]) // Stub - needs kernel implementation
    }

    fn mod_small(&self, x: [u32;8], modulus: u32) -> Result<u32> {
        // CUDA modular reduction to small modulus
        Ok(0u32) // Stub - needs kernel implementation
    }

    fn batch_mod_small(&self, points: &Vec<[[u32;8];3]>, modulus: u32) -> Result<Vec<u32>> {
        // CUDA batch modular reduction
        Ok(vec![0u32; points.len()]) // Stub - needs kernel implementation
    }

    fn rho_walk(&self, tortoise: [[u32;8];3], hare: [[u32;8];3], max_steps: u32) -> Result<RhoWalkResult> {
        // CUDA rho walk implementation
        Ok(RhoWalkResult {
            cycle_len: 42,
            cycle_point: tortoise,
            cycle_dist: [0u32; 8],
        }) // Stub - needs kernel implementation
    }

    fn solve_post_walk(&self, walk: RhoWalkResult, targets: Vec<[[u32;8];3]>) -> Result<Option<[u32;8]>> {
        // CUDA post-walk solving
        Ok(Some([42, 0, 0, 0, 0, 0, 0, 0])) // Stub - needs kernel implementation
    }


    fn generate_preseed_pos(&self, range_min: &BigInt256, range_width: &BigInt256) -> Result<Vec<f64>> {
        crate::utils::bias::generate_preseed_pos(range_min, range_width).map_err(Into::into)
    }

    fn blend_proxy_preseed(&self, preseed_pos: Vec<f64>, num_random: usize, empirical_pos: Option<Vec<f64>>, weights: (f64, f64, f64)) -> Result<Vec<f64>> {
        crate::utils::bias::blend_proxy_preseed(preseed_pos, num_random, empirical_pos, weights, false).map_err(Into::into)
    }

    fn analyze_preseed_cascade(&self, proxy_pos: &[f64], bins: usize) -> Result<(Vec<f64>, Vec<f64>)> {
        Ok(crate::utils::bias::analyze_preseed_cascade(proxy_pos, bins))
    }

    // Helper functions for CUDA backend
    fn point_to_u32_array_cuda(point: &crate::types::Point) -> [[u32;8];3] {
        // Convert Point coordinates to u32 arrays
        let x_bytes = point.x.to_bytes_be();
        let y_bytes = point.y.to_bytes_be();
        let z_bytes = point.z.to_bytes_be();

        [bytes_to_u32_array_cuda(&x_bytes), bytes_to_u32_array_cuda(&y_bytes), bytes_to_u32_array_cuda(&z_bytes)]
    }

    fn bytes_to_u32_array_cuda(bytes: &[u8; 32]) -> [u32;8] {
        let mut result = [0u32; 8];
        for i in 0..8 {
            let start = i * 4;
            result[i] = u32::from_be_bytes(bytes[start..start+4].try_into().unwrap());
        }
        result
    }

    fn bytes_from_u32_array(arr: &[u32;8]) -> [u8;32] {
        let mut bytes = [0u8; 32];
        for i in 0..8 {
            let start = i * 4;
            bytes[start..start+4].copy_from_slice(&arr[i].to_be_bytes());
        }
        bytes
    }
}

/// Set up SoA (Struct of Arrays) memory layout for better coalescing
#[cfg(feature = "rustacuda")]
pub fn create_soa_layout(num_kangaroos: usize) -> Result<SoaLayout, DriverError> {
    // Allocate separate arrays for each BigInt256 component
    let x_limbs = DeviceBuffer::from_slice(&vec![0u32; num_kangaroos * 4])?;
    let y_limbs = DeviceBuffer::from_slice(&vec![0u32; num_kangaroos * 4])?;
    let z_limbs = DeviceBuffer::from_slice(&vec![0u32; num_kangaroos * 4])?;
    let dist_limbs = DeviceBuffer::from_slice(&vec![0u32; num_kangaroos * 4])?;

    Ok(SoaLayout {
        x_limbs,
        y_limbs,
        z_limbs,
        dist_limbs,
        num_kangaroos,
    })
}

/// CPU fallback when CUDA is not available
#[cfg(not(feature = "rustacuda"))]
pub struct CudaBackend;

#[cfg(not(feature = "rustacuda"))]
#[async_trait::async_trait]
